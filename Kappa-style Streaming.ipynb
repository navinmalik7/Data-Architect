# Kappa-style: single streaming path with replayable input
from pyspark.sql import SparkSession
from pyspark.sql.functions import window, col, avg, count
from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType

spark = (SparkSession.builder
         .appName("KappaPipeline")
         .getOrCreate())

schema = StructType([
    StructField("device_id", StringType()),
    StructField("ts", TimestampType()),
    StructField("temp_c", DoubleType()),
    StructField("status", StringType()),
])

# Example inputs:
# 1) From files landing in a directory (good for demos):
#    .format("json").schema(schema).load("/mnt/data/iot/stream/")
# 2) From Kafka/Event Hubs (prod-like):
#    .format("kafka").option("kafka.bootstrap.servers", "...").option("subscribe", "iot").load()

raw = (spark.readStream
       .format("json")
       .schema(schema)
       .load("/mnt/data/iot/stream/"))  # replace with your landing path

# Basic cleaning & filtering
clean = (raw
         .withWatermark("ts", "10 minutes")
         .filter(col("temp_c").isNotNull()))

# Windowed aggregates
agg = (clean
       .groupBy(
           window(col("ts"), "5 minutes"),
           col("device_id")
       )
       .agg(
           avg("temp_c").alias("avg_temp_c"),
           count("*").alias("event_count")
       ))

# Write to Delta as the single materialized view (serving table)
query = (agg.writeStream
         .format("delta")
         .outputMode("append")
         .option("checkpointLocation", "/mnt/checkpoints/kappa/agg")
         .option("path", "/mnt/delta/kappa/device_5min_agg")
         .start())

# Later, query this table (in a separate notebook/cell):
# spark.read.format("delta").load("/mnt/delta/kappa/device_5min_agg").createOrReplaceTempView("device_5min_agg")
# spark.sql("SELECT * FROM device_5min_agg ORDER BY window.start DESC, device_id LIMIT 20").show()
